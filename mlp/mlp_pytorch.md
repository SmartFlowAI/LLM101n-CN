ä½œè€…ï¼šæ ¼é™µå…°å²›çš„è™

å¼•è¨€ï¼šæ¬¢è¿åŠ å…¥ OpenAI è”åˆåˆ›å§‹äººç¡¬æ ¸æ¨å‡ºçš„ LLM101n è¯¾ç¨‹çš„ä¸­æ–‡ç‰ˆå…±å»ºå…±å­¦è®¡åˆ’ï¼æœ¬æœŸé‡ç£…æ¨å‡ºé’ˆå¯¹ mlp æ¨¡å—çš„ pytorch ç‰ˆå®ç°ä»£ç çš„æ·±åº¦è§£è¯»

LLM101n æ˜¯ OpenAI è”åˆåˆ›å§‹äººã€â€œè®¡ç®—æœºè§†è§‰æ•™æ¯â€æé£é£æ•™æˆçš„é«˜å¾’Andrej Karpathy æ¨å‡ºçš„â€œä¸–ç•Œä¸Šæ˜¾ç„¶æœ€å¥½çš„ AI è¯¾ç¨‹â€ã€‚æˆ‘ä»¬é‚€è¯·äº†ã€Œæœºæ™ºæµã€çš„ç¤¾åŒºåŒå­¦ï¼Œå¹¶åˆ¶ä½œäº†æœ¬æœŸé’ˆå¯¹ LLM101n ä¸­å…³äºå¦‚ä½•åŸºäº pytorch å®ç°å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„æ·±åº¦è§£è¯»ã€‚æˆ‘ä»¬åç»­è¿˜ä¼šæ›´æ–°å…³äºè¯¥è¯¾ç¨‹æ ¸å¿ƒä»£ç çš„è§£è¯»ï¼Œæ¬¢è¿å…³æ³¨ã€‚

ä¸­æ–‡ç‰ˆå…±å»ºä»“åº“ï¼š

https://github.com/SmartFlowAI/LLM101n-CN

å®Œæ•´ä»£ç ï¼š

https://github.com/EurekaLabsAI/mlp/blob/master/mlp_pytorch.py

LLM101n åŸå§‹ä»“åº“ï¼š

https://github.com/EurekaLabsAI

---

ä»£ç ç›®å½•ç»“æ„æ ‘ï¼š

```
mlp
|-- README.md
|-- common.py
|-- data
|   |-- preprocess.py
|   |-- test.txt
|   |-- train.txt
|   `-- val.txt
|-- mlp_numpy.py
`-- mlp_pytorch.py
```

ä»Šå¤©å°†å’Œå¤§å®¶ä¸€èµ·å­¦ä¹  LLM101n è¯¾ç¨‹ä¸­ MLP éƒ¨åˆ†çš„ Python æ ¸å¿ƒä»£ç ï¼ˆpytorchç‰ˆï¼‰ï¼Œå³ä¸Šé¢ğŸ‘†ç»“æ„æ ‘ä¸­çš„ `mlp_pytorch.py`ã€‚å¤§å®¶å¯ä»¥ä½¿ç”¨`git clone`å‘½ä»¤å…‹éš†å¥½ä»“åº“ï¼Œç»“åˆæºä»£ç å’Œæœ¬è§£è¯»ä¸€èµ·é£Ÿç”¨æ›´ä½³å“¦~

é˜…è¯» Tipsï¼šæœ¬æ–‡ä»£ç å—çš„å‡ ä¹æ¯ä¸€å¥éƒ½æœ‰ç®€çŸ­çš„æ³¨é‡Šå“¦~

# ä»£ç è§£è¯»

## ä»£ç æ•´ä½“æ¡†æ¶

å¦‚ä»£ç æ•´ä½“æ¡†æ¶å›¾æ‰€ç¤ºï¼Œä»£ç å¯ä»¥æ‹†è§£ä¸ºä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š



![](https://files.mdnice.com/user/58235/9422ccd2-a724-4272-b7fd-a4d9f89dad11.PNG)


-   dataloader æ¨¡å—ï¼šä¸ºæ¨¡å‹çš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•åŠ è½½æ•°æ®ã€‚
-   æ¨¡å‹å®šä¹‰æ¨¡å—ï¼šå®šä¹‰ MLP æ¨¡å‹ï¼ŒåŒ…æ‹¬äº†ä½¿ç”¨`nn.Module`å’Œ ä¸ä½¿ç”¨`nn.Module`ä¸¤ç§æ¨¡å‹ç‰ˆæœ¬ã€‚åœ¨è¯¥æ¨¡å—ä¸‹ï¼Œä½¿ç”¨äº†è‡ªå®šä¹‰çš„ RNG éšæœºæ•°æ¨¡å—æ¥è¿›è¡Œæ¨¡å‹å‚æ•°çš„åˆå§‹åŒ–ã€‚
-   æ¨¡å‹è®­ç»ƒæ¨¡å—ï¼šä½¿ç”¨è®­ç»ƒé›†è®­ç»ƒ NLP æ¨¡å‹ã€‚
-   è¯„ä¼°æ¨¡å—ï¼šè¯„ä¼°æ¨¡å‹è¡¨ç°ã€‚
-   æ¨¡å‹æ¨ç†æ¨¡å—ï¼šè¿›è¡Œå‰å‘æ¨ç†ã€‚
-   RNG éšæœºæ•°æ¨¡å—ï¼šæ˜¯ä¸€ä¸ªè‡ªå®šä¹‰çš„éšæœºæ•°æ¨¡å—ï¼Œç”¨äºæ§åˆ¶éšæœºæ•°çš„ç”Ÿæˆå’Œæ¨¡å‹å‚æ•°åˆå§‹åŒ–ï¼Œä¿è¯å®éªŒçš„é‡å¤æ€§ã€‚

ä¸‹é¢ï¼Œæˆ‘ä»¬å°†ä»è¿™äº›æ¨¡å—çš„åŸºç¡€ä¸Šå‡ºå‘è§£è¯»ä»£ç ã€‚

> æ³¨ï¼šä½¿ç”¨åˆ°çš„ç¬¬ä¸‰æ–¹åº“ï¼šmathã€timeã€torch

## dataloader

å®šä¹‰äº†ä¸€ä¸ªåä¸º dataloader çš„å‡½æ•°ï¼Œå…¶æ¥å—ä¸‰ä¸ªå‚æ•°ï¼š

-   tokensï¼šä¸€ä¸ªåŒ…å«æ‰€æœ‰ token çš„åˆ—è¡¨æˆ–æ•°ç»„ã€‚
-   context_lengthï¼šä¸Šä¸‹æ–‡çš„é•¿åº¦ï¼Œå³æ¯æ¬¡è¾“å…¥çš„ token æ•°é‡ã€‚
-   batch_sizeï¼šæ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°ã€‚

å…¶å®šä¹‰ä»£ç å¦‚ä¸‹ï¼š

```python
def dataloader(tokens, context_length, batch_size):
    # returns inputs, targets as torch Tensors of shape (B, T), (B, )
    n = len(tokens) # è®¡ç®— tokens çš„é•¿åº¦ nï¼Œç”¨äºåç»­çš„éå†ã€‚
    inputs, targets = [], [] # åˆ›å»ºç©ºçš„åˆ—è¡¨ inputs å’Œ targets ç”¨äºå­˜å‚¨è¾“å…¥æ•°æ®å’Œç›®æ ‡æ•°æ®ã€‚
    pos = 0 # å®šä¹‰ pos å˜é‡ï¼Œè¡¨ç¤ºå½“å‰çª—å£çš„èµ·å§‹ä½ç½®ã€‚
    while True: # è¿›å…¥ä¸€ä¸ª while å¾ªç¯ï¼Œç”¨äºä¸æ–­ç”Ÿæˆæ‰¹æ¬¡æ•°æ®ã€‚
        # simple sliding window over the tokens, of size context_length + 1
        window = tokens[pos:pos + context_length + 1] # å–ä»å½“å‰ pos å¼€å§‹çš„ context_length + 1 ä¸ª token ä½œä¸ºçª—å£ã€‚
        inputs.append(window[:-1]) # å–çª—å£ä¸­çš„å‰ context_length ä¸ª token ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ° inputs åˆ—è¡¨ä¸­ã€‚
        targets.append(window[-1]) # å–çª—å£ä¸­çš„æœ€åä¸€ä¸ª token ä½œä¸ºç›®æ ‡ï¼Œå¹¶å°†å®ƒæ·»åŠ åˆ° targets åˆ—è¡¨ä¸­ã€‚
        # once we've collected a batch, emit it
        if len(inputs) == batch_size: # å½“ inputs åˆ—è¡¨çš„é•¿åº¦ç­‰äº batch_size æ—¶ï¼Œç”Ÿæˆå½“å‰æ‰¹æ¬¡çš„è¾“å…¥å’Œç›®æ ‡å¼ é‡ã€‚
            yield (torch.tensor(inputs), torch.tensor(targets)) # ä½¿ç”¨ yield å…³é”®å­—è¿”å›å®ƒä»¬ã€‚æ­¤æ—¶ dataloader å‡½æ•°æˆä¸ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æŒ‰éœ€æä¾›æ•°æ®ã€‚
            inputs, targets = [], [] # é‡ç½® inputs å’Œ targets åˆ—è¡¨ä»¥æ”¶é›†ä¸‹ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ã€‚
        # advance the position and wrap around if we reach the end
        pos += 1 # å°† pos å‰ç§»ä¸€ä¸ª tokenã€‚
        if pos + context_length >= n: # å¦‚æœ pos åŠ ä¸Š context_length è¶…å‡ºäº† tokens çš„é•¿åº¦ï¼Œåˆ™å°† pos é‡ç½®ä¸º 0ï¼Œä»å¤´å¼€å§‹å¾ªç¯ã€‚
            pos = 0
```

`dataloader`å‡½æ•°é€šè¿‡æ»‘åŠ¨çª—å£çš„æ–¹æ³•ï¼Œç”Ÿæˆä¸€ç³»åˆ—è¾“å…¥å’Œç›®æ ‡å¯¹ï¼Œå¹¶æŒ‰æ‰¹æ¬¡å¤§å°ç”Ÿæˆæ•°æ®ã€‚

> åè®°ï¼šä¹‹æ‰€ä»¥è¿™æ®µä»£ç ä¸­çš„ `while` å¾ªç¯ä½“ä¸éœ€è¦é€€å‡ºæ¡ä»¶ï¼Œæ˜¯å› ä¸ºå…¶ä¸­çš„ `yield`è¯­å¥åœ¨å‡½æ•°ä¸­åˆ›å»ºäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼ˆgeneratorï¼‰ï¼Œå®ƒå¯ä»¥æš‚åœå‡½æ•°çš„æ‰§è¡Œå¹¶è¿”å›ä¸€ä¸ªå€¼ï¼ŒåŒæ—¶ä¿ç•™å‡½æ•°çš„çŠ¶æ€ã€‚ä¸‹æ¬¡è¿­ä»£æ—¶ï¼Œå‡½æ•°ä¼šä»ä¸Šæ¬¡æš‚åœçš„åœ°æ–¹ç»§ç»­æ‰§è¡Œï¼Œè€Œä¸æ˜¯é‡æ–°å¼€å§‹ã€‚è¿™æ ·å°±å¯ä»¥å®ç°æŒ‰éœ€ç”Ÿæˆæ•°æ®æ‰¹æ¬¡ï¼Œè€Œä¸éœ€è¦ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®ï¼ŒèŠ‚çœäº†å†…å­˜ç©ºé—´å¹¶æé«˜äº†æ•ˆç‡ã€‚

# æ¨¡å‹å®šä¹‰

ä½œè€… Andrej Karpathy åœ¨è¯¥ä»“åº“ä¸­ä½¿ç”¨äº†ä¸¤ç§æ–¹å¼æ¥æ„å»º MLP æ¨¡å‹ï¼ŒåŒºåˆ«åœ¨äºå®ç°è¿‡ç¨‹ä¸­æ˜¯å¦ä½¿ç”¨`torch.nn.Module`æ¨¡å—ï¼Œä½†ä¸¤ç§æ–¹æ³•çš„ä¸€èˆ¬æ€è·¯éƒ½æ˜¯åˆå§‹åŒ–æ¨¡å‹åŠå…¶å‚æ•°ï¼ˆå®ç°`__init__()`ï¼‰å¹¶å®šä¹‰å‰å‘ä¼ æ’­å‡½æ•°ï¼ˆå®ç°`forward()`ï¼‰è®¡ç®—è¾“å‡ºã€‚

## MLPRaw

æˆ‘ä»¬å…ˆä¸ä½¿ç”¨`nn.Module`è€Œæ˜¯é€šè¿‡æ‰‹åŠ¨åˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼ˆå¦‚æƒé‡å’Œåç½®ï¼‰çš„æ–¹æ³•æ¥å®šä¹‰ä¸€ä¸ª MLPRaw æ¨¡å‹ç±»ã€‚

**é¦–å…ˆ**ï¼Œæ‰‹åŠ¨åˆå§‹åŒ–æ¨¡å‹åŠå…¶å‚æ•°ï¼ˆä¹Ÿå°±æ˜¯æŠŠæ­å»ºæ¨¡å‹éœ€è¦çš„ç§¯æœ¨å‡†å¤‡å¥½ï¼‰ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
class MLPRaw: 
    def __init__(self, vocab_size, context_length, embedding_size, hidden_size, rng):       
        # æ¥å—äº”ä¸ªå‚æ•°ï¼š
        ## vocab_sizeï¼ˆè¯æ±‡è¡¨å¤§å°ï¼‰
        ## context_lengthï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰
        ## embedding_sizeï¼ˆåµŒå…¥å±‚å¤§å°ï¼‰
        ## hidden_sizeï¼ˆéšè—å±‚å¤§å°ï¼‰
        ## rngï¼ˆéšæœºæ•°ç”Ÿæˆå™¨ï¼Œè¿™ä¸ªä¼šåœ¨åé¢è¯¦ç»†ä»‹ç»ï¼‰ ã€‚
        v, t, e, h = vocab_size, context_length, embedding_size, hidden_size    # é€šè¿‡å˜é‡ vã€tã€e å’Œ h åˆ†åˆ«è¡¨ç¤ºè¯æ±‡è¡¨å¤§å°ã€ä¸Šä¸‹æ–‡é•¿åº¦ã€åµŒå…¥å±‚å¤§å°å’Œéšè—å±‚å¤§å°ã€‚        
        self.embedding_size = embedding_size # ä¿å­˜ embedding_size ä¸ºç±»çš„å±æ€§ã€‚
        # self.wte è¡¨ç¤ºåµŒå…¥å±‚æƒé‡ã€‚
        ## å…ˆä½¿ç”¨éšæœºæ•°ç”Ÿæˆå™¨ rng ç”Ÿæˆä¸€ä¸ªæ­£æ€åˆ†å¸ƒ N(0, 1) çš„éšæœºæ•°ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º PyTorch å¼ é‡ã€‚
        ## å†å°†å¼ é‡è°ƒæ•´ä¸ºå½¢çŠ¶ (v, e)ï¼Œè¡¨ç¤ºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªè¯éƒ½æœ‰ä¸€ä¸ª embedding_size ç»´åº¦çš„åµŒå…¥å‘é‡ã€‚
        self.wte = torch.tensor(rng.randn(v * e, mu=0, sigma=1.0)).view(v, e) 
        scale = 1 / math.sqrt(e * t) # è®¡ç®—ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„ç¼©æ”¾ç³»æ•° scaleï¼Œå€¼ä¸º 1 / sqrt(e * t)ã€‚
        # self.fc1_weights è¡¨ç¤ºç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„æƒé‡ï¼Œself.fc1_bias è¡¨ç¤ºç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„åç½®ã€‚
        ## ä½¿ç”¨éšæœºæ•°ç”Ÿæˆå™¨ rng ç”Ÿæˆå‡åŒ€åˆ†å¸ƒ U(-scale, scale) çš„ t * e * h ä¸ªéšæœºæ•°ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º PyTorch å¼ é‡ã€‚
        ## å°†å¼ é‡è°ƒæ•´ä¸ºå½¢çŠ¶ (h, t * e) å¹¶è½¬ç½®ï¼Œè¡¨ç¤ºç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„æƒé‡ã€‚
        self.fc1_weights =  torch.tensor(rng.rand(t * e * h, -scale, scale)).view(h, t * e).T
        self.fc1_bias = torch.tensor(rng.rand(h, -scale, scale))
        scale = 1 / math.sqrt(h) # è®¡ç®—ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚çš„ç¼©æ”¾ç³»æ•° scaleï¼Œå€¼ä¸º 1 / sqrt(h)ã€‚
        # self.fc2_weights è¡¨ç¤ºç¬¬äºŒä¸ªå…¨è¿æ¥å±‚çš„æƒé‡ï¼Œself.fc2_bias è¡¨ç¤ºç¬¬äºŒä¸ªå…¨è¿æ¥å±‚çš„åç½®ã€‚
        ## å‚æ•°åˆå§‹åŒ–æ–¹å¼å¦‚ fc1
        self.fc2_weights = torch.tensor(rng.rand(v * h, -scale, scale)).view(v, h).T
        self.fc2_bias = torch.tensor(rng.rand(v, -scale, scale))
        for p in self.parameters(): # éå† parameters æ–¹æ³•è¿”å›çš„æ‰€æœ‰æ¨¡å‹å‚æ•°ï¼Œå°†å®ƒä»¬çš„ requires_grad å±æ€§è®¾ç½®ä¸º Trueï¼Œè¡¨ç¤ºè¿™äº›å‚æ•°éœ€è¦è®¡ç®—æ¢¯åº¦ã€‚
            p.requires_grad = True
    def parameters(self):
        # å®šä¹‰ parameters æ–¹æ³•ï¼Œè¿”å›æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚
        return [self.wte, self.fc1_weights, self.fc1_bias, self.fc2_weights, self.fc2_bias]
```

åˆå§‹åŒ–ç½‘ç»œå‚æ•°ä½¿ç”¨åˆ°çš„éšæœºæ•°ç”Ÿæˆå™¨ rng å®šä¹‰åœ¨`common.py`æ–‡ä»¶ä¸­ï¼Œè¯¥ RNG ç±»æä¾›äº†ä¸€ä¸ªå®Œå…¨ç¡®å®šæ€§çš„éšæœºæ•°ç”Ÿæˆå™¨ï¼Œå®ç°äº†å‡åŒ€åˆ†å¸ƒæˆ–æ­£æ€åˆ†å¸ƒéšæœºæ•°çš„ç”Ÿæˆã€‚é€šè¿‡ Box-Muller å˜æ¢å’Œ`xorshift*`ç®—æ³•å®ç°ç”Ÿæˆæ­£æ€åˆ†å¸ƒå’Œå‡åŒ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œç¡®ä¿äº†ç”Ÿæˆå™¨çš„å¯é‡å¤æ€§å’Œå¯æ§æ€§ã€‚

RNG ç±»ä»£ç å¦‚ä¸‹ï¼š

```python
def box_muller_transform(u1, u2):
    # å®ç°äº†åŸºæœ¬å½¢å¼çš„ Box-Muller å˜æ¢ï¼Œç”¨äºå°†ä¸¤ä¸ª [0, 1) åŒºé—´ä¸Šçš„å‡åŒ€éšæœºæ•° u1 å’Œ u2ï¼Œ
    # è½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„éšæœºæ•° z1 å’Œ z2ã€‚
    z1 = (-2 * log(u1)) ** 0.5 * cos(2 * pi * u2)
    z2 = (-2 * log(u1)) ** 0.5 * sin(2 * pi * u2)
    return z1, z2

class RNG:
    def __init__(self, seed):
        # æ¥å—ä¸€ä¸ªç§å­ seedï¼Œç”¨äºåˆå§‹åŒ–éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€ self.stateï¼Œä½¿å¾—ç”Ÿæˆå™¨æ˜¯ç¡®å®šæ€§çš„ã€‚
        self.state = seed

    def random_u32(self):
        # å®ç°äº† xorshift* éšæœºæ•°ç”Ÿæˆç®—æ³•ã€‚
        ## ä½¿ç”¨æŒ‰ä½æ“ä½œï¼ˆ^ å’Œ >>, <<ï¼‰æ›´æ–° self.stateã€‚
        ## è¿”å›ä¸€ä¸ª 32 ä½æ— ç¬¦å·æ•´æ•°ï¼Œç¡®ä¿è¾“å‡ºåœ¨ [0, 2^32-1] èŒƒå›´å†…ã€‚
        # ä½¿ç”¨ & 0xFFFFFFFFFFFFFFFF ç¡®ä¿ç»“æœåœ¨ 64 ä½èŒƒå›´å†…ï¼ˆç±»ä¼¼äºåœ¨ C ä¸­å°†ç»“æœå¼ºåˆ¶è½¬æ¢ä¸º uint64ï¼‰ã€‚
        self.state ^= (self.state >> 12) & 0xFFFFFFFFFFFFFFFF 
        # ä¸Šä¸€æ­¥çš„æ“ä½œï¼š
        ## 1.å³ç§» self.state 12 ä½ï¼Œå¹¶ä¸ 0xFFFFFFFFFFFFFFFF æŒ‰ä½ä¸ï¼Œç¡®ä¿ç»“æœåœ¨ 64 ä½èŒƒå›´å†…ã€‚
        ## 2.ä½¿ç”¨æŒ‰ä½å¼‚æˆ–æ“ä½œ ^= æ›´æ–° self.stateã€‚
        ## è¿™ä¸€æ“ä½œæ··åˆäº† self.state çš„é«˜ä½å’Œä½ä½ï¼Œå¢åŠ äº†éšæœºæ€§ã€‚ 
        self.state ^= (self.state << 25) & 0xFFFFFFFFFFFFFFFF
        self.state ^= (self.state >> 27) & 0xFFFFFFFFFFFFFFFF
        # ä½¿ç”¨ & 0xFFFFFFFF ç¡®ä¿ç»“æœåœ¨ 32 ä½èŒƒå›´å†…ï¼ˆç±»ä¼¼äºåœ¨ C ä¸­å°†ç»“æœå¼ºåˆ¶è½¬æ¢ä¸º uint32ï¼‰ã€‚
        ## å°† self.state ä¹˜ä»¥ä¸€ä¸ªå¸¸æ•° 0x2545F4914F6CDD1Dã€‚
        ## è¿™ä¸ªå¸¸æ•°æ˜¯ç»è¿‡é€‰æ‹©çš„ï¼Œå¯ä»¥ç¡®ä¿ç”Ÿæˆçš„éšæœºæ•°å…·æœ‰è‰¯å¥½çš„åˆ†å¸ƒç‰¹æ€§ã€‚
        return ((self.state * 0x2545F4914F6CDD1D) >> 32) & 0xFFFFFFFF

    def random(self):
        # å°† random_u32 ç”Ÿæˆçš„ 32 ä½æ— ç¬¦å·æ•´æ•°å³ç§» 8 ä½ï¼Œå¹¶é™¤ä»¥ 2^24ï¼ˆ16777216.0ï¼‰
        # å¾—åˆ° [0, 1) åŒºé—´çš„æµ®ç‚¹æ•°ã€‚
        return (self.random_u32() >> 8) / 16777216.0

    def rand(self, n, a=0, b=1):
        # ç”Ÿæˆ n ä¸ª [a, b) åŒºé—´çš„å‡åŒ€éšæœºæ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ªåˆ—è¡¨ã€‚
        ## ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼è°ƒç”¨ random å‡½æ•°ç”Ÿæˆ n ä¸ªéšæœºæ•°ï¼Œå¹¶å°†å®ƒä»¬çº¿æ€§æ˜ å°„åˆ° [a, b) åŒºé—´ã€‚
        return [self.random() * (b - a) + a for _ in range(n)]

    def randn(self, n, mu=0, sigma=1):
        # ç”Ÿæˆ n ä¸ªæœä»æ­£æ€åˆ†å¸ƒ N(mu, sigma^2) çš„éšæœºæ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ªåˆ—è¡¨ã€‚
        out = []
        for _ in range((n + 1) // 2):
            u1, u2 = self.random(), self.random()
            z1, z2 = box_muller_transform(u1, u2) # # ä½¿ç”¨ Box-Muller å˜æ¢ç”Ÿæˆä¸¤ä¸ªæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„éšæœºæ•° z1 å’Œ z2ï¼Œå¹¶å°†å®ƒä»¬æ‰©å±•åˆ°è¾“å‡ºåˆ—è¡¨ outã€‚
            out.extend([z1 * sigma + mu, z2 * sigma + mu]) # ä¹˜ä»¥ sigma å¹¶åŠ ä¸Š mu ä»¥è°ƒæ•´åˆ°æœŸæœ›çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚
        out = out[:n] # å¦‚æœ n æ˜¯å¥‡æ•°ï¼Œæˆªæ–­åˆ—è¡¨ out ä»¥ç¡®ä¿è¿”å› n ä¸ªéšæœºæ•°ã€‚
        return out
```

æ¥ä¸‹æ¥æˆ‘ä»¬å†å›è¿‡å¤´å…³æ³¨ä¸€ä¸‹ MLPRaw æ¨¡å‹ç±»çš„åˆå§‹åŒ–çš„å‡ ä¸ª Q&Aï¼š

**Q1ï¼šä¸ºä»€ä¹ˆç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚å’Œç¬¬äºŒä¸ªå…¨è¿æ¥å±‚çš„ scale å€¼ä¸åŒï¼Ÿ**

Aï¼šå› ä¸ºå®ƒä»¬çš„è¾“å…¥ç‰¹å¾æ•°é‡ä¸åŒã€‚å¸¸è§çš„åˆå§‹åŒ–ç­–ç•¥å³æ ¹æ®è¾“å…¥ç‰¹å¾çš„æ•°é‡æ¥è°ƒæ•´æƒé‡çš„åˆå§‹åŒ–èŒƒå›´ï¼Œè¿™æ ·å¯ä»¥ä¿æŒå‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­è¾“å…¥å’Œè¾“å‡ºçš„æ–¹å·®ç›¸å¯¹ç¨³å®šï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±ã€‚ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚è¾“å…¥ç‰¹å¾æ•°é‡æ˜¯context_length * embedding_sizeï¼Œå³ T * eï¼›ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚è¾“å…¥ç‰¹å¾æ•°é‡æ˜¯ hidden_sizeï¼Œå³ hã€‚å› æ­¤æœ€ç»ˆä¸¤è€…çš„ scale åˆ†åˆ«æ˜¯ $\frac{1}{\sqrt{(e * t)}}$ å’Œ $\frac{1}{\sqrt h}$ã€‚

**Q2ï¼šä¸ºä»€ä¹ˆéœ€è¦éå†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œå°†å®ƒä»¬çš„ requires_grad å±æ€§è®¾ç½®ä¸º Trueï¼Ÿ**

Aï¼šå°†æ¨¡å‹å‚æ•°çš„ requires_grad å±æ€§è®¾ç½®ä¸º True æ˜¯ä¸ºäº†ç¡®ä¿åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è¿™äº›å‚æ•°çš„æ¢¯åº¦ä¼šè¢«è®¡ç®—å’Œå­˜å‚¨ï¼Œä»è€Œèƒ½å¤Ÿè¿›è¡Œæ¢¯åº¦æ›´æ–°ã€‚åœ¨æ‰‹åŠ¨å®ç°æ¨¡å‹æ—¶ï¼Œéœ€è¦æ˜¾å¼åœ°å°†è¿›è¡Œè¯¥è®¾ç½®ã€‚

**æ¥ä¸‹æ¥**ï¼Œå®šä¹‰å‰å‘ä¼ æ’­å‡½æ•°ï¼ˆä¹Ÿå°±æ˜¯æŠŠå‡†å¤‡å¥½çš„ç§¯æœ¨æŒ‰é¢„æƒ³çš„é¡ºåºæ­å»ºèµ·æ¥ï¼‰ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
def forward(self, idx, targets=None):
    # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³• forwardï¼Œæ¥å—ä¸¤ä¸ªå‚æ•°ï¼š
    ## idxï¼ˆè¾“å…¥ token çš„ç´¢å¼•ï¼‰å’Œ targetsï¼ˆç›®æ ‡ token çš„ç´¢å¼•ï¼Œå¯é€‰ï¼‰ã€‚
    # idx are the input tokens, (B, T) tensor of integers
    # targets are the target tokens, (B, ) tensor of integers
    B, T = idx.size() # è·å–è¾“å…¥ idx çš„å½¢çŠ¶ï¼ŒB è¡¨ç¤ºæ‰¹æ¬¡å¤§å°ï¼ŒT è¡¨ç¤ºä¸Šä¸‹æ–‡é•¿åº¦ã€‚
    # forward pass
    # ä½¿ç”¨åµŒå…¥å±‚ self.wte å°†è¾“å…¥ idx è½¬æ¢ä¸ºåµŒå…¥å‘é‡ã€‚
    emb = self.wte[idx] # (B, T, embedding_size) 
    # å°†åµŒå…¥å‘é‡å±•å¹³ã€‚
    emb = emb.view(B, -1) # (B, T * embedding_size)
    # é€šè¿‡ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚å’Œ tanh æ¿€æ´»å‡½æ•°ï¼Œè®¡ç®—éšè—å±‚çš„è¾“å‡º hiddenã€‚
    hidden = torch.tanh(emb @ self.fc1_weights + self.fc1_bias)
    # é€šè¿‡ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚è®¡ç®—è¾“å‡º logitsã€‚
    ## ç»“æœ logits çš„å½¢çŠ¶ä¸º (B, vocab_size)ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥åºåˆ—åœ¨è¯æ±‡è¡¨ä¸­æ¯ä¸ªè¯çš„å¾—åˆ†ã€‚
    logits = hidden @ self.fc2_weights + self.fc2_bias
    # å¦‚æœæä¾›äº†ç›®æ ‡ targetsï¼Œè®¡ç®—äº¤å‰ç†µæŸå¤± F.cross_entropy(logits, targets)ã€‚
    loss = None
    if targets is not None:
        loss = F.cross_entropy(logits, targets)
    return logits, loss
def __call__(self, idx, targets=None):
    # å®šä¹‰ __call__ æ–¹æ³•ï¼Œä½¿å¾—æ¨¡å‹å®ä¾‹å¯ä»¥åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨ã€‚
    return self.forward(idx, targets)
```

æ³¨ï¼šhidden @ self.fc2_weights æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œé€‚ç”¨äºç¬¦åˆçŸ©é˜µä¹˜æ³•è§„åˆ™çš„å¼ é‡ï¼›hidden * self.fc2_weights æ˜¯é€å…ƒç´ ç›¸ä¹˜ï¼Œè¦æ±‚ä¸¤ä¸ªæ“ä½œæ•°çš„å½¢çŠ¶å®Œå…¨ç›¸åŒã€‚åœ¨`forward`æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œçš„æ˜¯çŸ©é˜µä¹˜æ³•è€Œä¸æ˜¯é€å…ƒç´ ç›¸ä¹˜ã€‚

## MLP

æ˜¯ä¸æ˜¯è§‰å¾—ä¸Šé¢ğŸ‘†å®šä¹‰ MLPRaw æ¨¡å‹ç±»çš„æ–¹æ³•æœ‰äº›è®¸çš„ç¹çï¼Œé‚£ä¹ˆæ¥ä¸‹æ¥è®©æˆ‘ä»¬ä¸€èµ·çœ‹ä¸€ä¸‹ä½œè€… Andrej Karpathy åŸºäº`torch.nn.Module`ç»™å‡ºçš„ç¬¬äºŒç§å®ç°æ–¹å¼ã€‚

`torch.nn.Module`æ˜¯ PyTorch ä¸­ç”¨äºå®šä¹‰å’Œç®¡ç†ç¥ç»ç½‘ç»œçš„åŸºç±»ï¼Œè¯¥ç±»æä¾›äº†çµæ´»çš„ç»“æ„æ¥å®šä¹‰ç½‘ç»œå±‚å’Œå®ç°å‰å‘ä¼ æ’­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç»§æ‰¿`nn.Module`ç±»æ¥æ„å»ºè‡ªå®šä¹‰çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ä½¿ç”¨`nn.Module`æ„å»ºæ¨¡å‹çš„ä¸€èˆ¬æ­¥éª¤åŒ…æ‹¬ï¼šå®šä¹‰ç»§æ‰¿è‡ª`nn.Module`çš„ç±»ã€åœ¨`init`æ–¹æ³•ä¸­åˆå§‹åŒ–æ¨¡å‹å±‚ã€åœ¨`forward`æ–¹æ³•ä¸­å®šä¹‰å‰å‘ä¼ æ’­é€»è¾‘ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
class MLP(nn.Module): # ç»§æ‰¿è‡ª nn.Moduleï¼Œè¿™æ˜¯æ‰€æœ‰ PyTorch æ¨¡å‹çš„åŸºç±»ã€‚
    def __init__(self, vocab_size, context_length, embedding_size, hidden_size, rng):
        # æ¥å—äº”ä¸ªå‚æ•°ï¼švocab_sizeï¼ˆè¯æ±‡è¡¨å¤§å°ï¼‰ã€context_lengthï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰ã€
        ## embedding_sizeï¼ˆåµŒå…¥å±‚å¤§å°ï¼‰ã€hidden_sizeï¼ˆéšè—å±‚å¤§å°ï¼‰å’Œ rngï¼ˆéšæœºæ•°ç”Ÿæˆå™¨ï¼‰ã€‚
        # è°ƒç”¨ super().__init__() åˆå§‹åŒ–çˆ¶ç±» nn.Moduleã€‚
        super().__init__()
        # å®šä¹‰ä¸€ä¸ªåµŒå…¥å±‚ self.wteï¼Œä½¿ç”¨ nn.Embedding å°†è¾“å…¥çš„ token ç´¢å¼•è½¬æ¢ä¸ºåµŒå…¥å‘é‡ã€‚
        ## vocab_size æ˜¯è¯æ±‡è¡¨çš„å¤§å°ï¼Œembedding_size æ˜¯åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚
        self.wte = nn.Embedding(vocab_size, embedding_size) 
        # ä½¿ç”¨ nn.Sequential å®šä¹‰ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼š
        #
        self.mlp = nn.Sequential(
            nn.Linear(context_length * embedding_size, hidden_size), # ç¬¬ä¸€å±‚å…¨è¿æ¥å±‚ï¼Œå°†è¾“å…¥çš„ä¸Šä¸‹æ–‡åµŒå…¥å‘é‡æ˜ å°„åˆ°éšè—å±‚ã€‚
            nn.Tanh(), # # Tanh æ¿€æ´»å‡½æ•°ã€‚
            nn.Linear(hidden_size, vocab_size) # ç¬¬äºŒå±‚çº¿æ€§å±‚ï¼Œå°†éšè—å±‚çš„è¾“å‡ºæ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°çš„è¾“å‡ºã€‚
        )
        self.reinit(rng) # è°ƒç”¨ reinit å‡½æ•°ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„éšæœºæ•°ç”Ÿæˆå™¨ rng åˆå§‹åŒ–æƒé‡ã€‚
        
    @torch.no_grad()
    def reinit(self, rng):
        # å®šä¹‰ reinit å‡½æ•°ï¼Œå¹¶ä½¿ç”¨ @torch.no_grad() è£…é¥°å™¨ï¼Œè¡¨ç¤ºåœ¨è¿™ä¸ªå‡½æ•°ä¸­ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ã€‚
        def reinit_tensor_randn(w, mu, sigma):
            # ä»¥æ­£æ€åˆ†å¸ƒ N(mu, sigma) åˆå§‹åŒ–å¼ é‡ w çš„æƒé‡ã€‚
            winit = torch.tensor(rng.randn(w.numel(), mu=mu, sigma=sigma))
            w.copy_(winit.view_as(w))

        def reinit_tensor_rand(w, a, b):
            # ä»¥å‡åŒ€åˆ†å¸ƒ U(a, b) åˆå§‹åŒ–å¼ é‡ w çš„æƒé‡ã€‚
            winit = torch.tensor(rng.rand(w.numel(), a=a, b=b))
            w.copy_(winit.view_as(w))

        # Let's match the PyTorch default initialization:
        # ä»¥å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º1çš„æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–åµŒå…¥å±‚ self.wte çš„æƒé‡ã€‚
        reinit_tensor_randn(self.wte.weight, mu=0, sigma=1.0)
        scale = (self.mlp[0].in_features)**-0.5 # ç®—ç¬¬ä¸€å±‚å…¨è¿æ¥å±‚çš„ç¼©æ”¾ç³»æ•° scaleï¼Œå…¶å€¼ä¸ºè¾“å…¥ç‰¹å¾æ•°é‡çš„è´Ÿå¹³æ–¹æ ¹ã€‚
        # ä»¥å‡åŒ€åˆ†å¸ƒ U(-scale, scale) åˆå§‹åŒ–ç¬¬ä¸€å±‚å…¨è¿æ¥çš„æƒé‡å’Œåç½®ã€‚
        reinit_tensor_rand(self.mlp[0].weight, -scale, scale) 
        reinit_tensor_rand(self.mlp[0].bias, -scale, scale)
        # å¯¹äºç¬¬äºŒå±‚å…¨è¿æ¥å±‚çš„å¤„ç†åŒä¸Š
        scale = (self.mlp[2].in_features)**-0.5
        reinit_tensor_rand(self.mlp[2].weight, -scale, scale)
        reinit_tensor_rand(self.mlp[2].bias, -scale, scale)
        
    def forward(self, idx, targets=None):
        # ä¸ MLPRaw ç±»çš„ forward å‡½æ•°åŸºæœ¬ç›¸åŒï¼Œä½†æ›´ç®€æ´ã€‚
        B, T = idx.size()
        emb = self.wte(idx) # (B, T, embedding_size)
        emb = emb.view(B, -1) # (B, T * embedding_size)
        logits = self.mlp(emb)
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits, targets)
        return logits, loss
```

**Qï¼šä¸ºä»€ä¹ˆéœ€è¦å®šä¹‰ reinit() å‡½æ•°ï¼Ÿ**

Aï¼šå®šä¹‰ reinit() å‡½æ•°çš„ä¸»è¦åŸå› æ˜¯ä¸ºäº†ä½¿ç”¨è‡ªå®šä¹‰çš„éšæœºæ•°ç”Ÿæˆå™¨æ¥åˆå§‹åŒ–æ¨¡å‹çš„æƒé‡å’Œåç½®ï¼Œç¡®ä¿åˆå§‹åŒ–çš„å¯æ§æ€§å’Œä¸€è‡´æ€§ï¼Œä»è€Œæé«˜å®éªŒçš„å¯é‡å¤æ€§å’Œç»“æœçš„å¯é æ€§ã€‚ä½†åœ¨æˆ‘ä»¬å¹³æ—¶ä¸€èˆ¬çš„å·¥ä½œä¸­å¯ä»¥ä¸ä½¿ç”¨ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ torchinfo å’Œ torchviz ç¬¬ä¸‰æ–¹åº“æ¥æ‰“å°æ¨¡å‹çš„ç»“æ„ç­‰ç›¸å…³ä¿¡æ¯ä»¥åŠå¯è§†åŒ–è®¡ç®—å›¾ï¼Œå¦‚ä¸‹æ‰€ç¤º~


![](https://files.mdnice.com/user/58235/ae2ca264-8f4f-4606-aa31-bc9594882be4.png)

![](https://files.mdnice.com/user/58235/463f8374-39be-4c03-a24d-66c2357ba1f3.png)


# æ¨¡å‹è®­ç»ƒ

## æ•°æ®å‡†å¤‡

è¯»å–è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®ï¼Œè¿›è¡Œå­—ç¬¦åˆ° token çš„æ˜ å°„ï¼Œå¹¶é¢„å¤„ç†æ•°æ®ä»¥ä¾¿åç»­ä½¿ç”¨ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
# "train" the Tokenizer, so we're able to map between characters and tokens
train_text = open('data/train.txt', 'r').read() # è¯»å–è®­ç»ƒæ•°æ®æ–‡ä»¶ train.txt çš„å†…å®¹ã€‚
assert all(c == '\n' or ('a' <= c <= 'z') for c in train_text) # æ–­è¨€æ£€æŸ¥æ‰€æœ‰å­—ç¬¦æ˜¯å¦ä¸ºå°å†™å­—æ¯æˆ–æ¢è¡Œç¬¦ï¼Œä»¥ç¡®ä¿æ•°æ®ç¬¦åˆé¢„æœŸæ ¼å¼ã€‚
uchars = sorted(list(set(train_text))) # æå–è¾“å…¥æ–‡æœ¬ä¸­çš„å”¯ä¸€å­—ç¬¦ï¼Œå¹¶æŒ‰å­—æ¯é¡ºåºæ’åºã€‚
vocab_size = len(uchars) # è®¡ç®—è¯æ±‡è¡¨å¤§å° vocab_sizeã€‚
# åˆ›å»ºå­—ç¬¦åˆ° token çš„æ˜ å°„ char_to_token å’Œ token åˆ°å­—ç¬¦çš„æ˜ å°„ token_to_charã€‚
char_to_token = {c: i for i, c in enumerate(uchars)} 
token_to_char = {i: c for i, c in enumerate(uchars)}
EOT_TOKEN = char_to_token['\n'] # æŒ‡å®šæ¢è¡Œç¬¦ \n ä¸ºç»“æŸç¬¦ EOT_TOKENã€‚
# å°†é¢„å…ˆåˆ’åˆ†å¥½çš„æµ‹è¯•æ•°æ®ã€éªŒè¯æ•°æ®å’Œè®­ç»ƒæ•°æ®åˆ†åˆ«é¢„å¤„ç†ä¸º token åˆ—è¡¨ã€‚
test_tokens = [char_to_token[c] for c in open('data/test.txt', 'r').read()]
val_tokens = [char_to_token[c] for c in open('data/val.txt', 'r').read()]
train_tokens = [char_to_token[c] for c in open('data/train.txt', 'r').read()]
```

## æ¨¡å‹å’Œä¼˜åŒ–å™¨çš„åˆ›å»º

æ ¹æ®æŒ‡å®šçš„å‚æ•°åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼Œå¹¶åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
# è®¾ç½®æ¨¡å‹å‚æ•°ï¼šä¸Šä¸‹æ–‡é•¿åº¦ context_lengthã€åµŒå…¥å±‚å¤§å° embedding_size å’Œéšè—å±‚å¤§å° hidden_sizeã€‚
context_length = 3 # if 3 tokens predict the 4th, this is a 4-gram model
embedding_size = 48
hidden_size = 512
# åˆ›å»ºéšæœºæ•°ç”Ÿæˆå™¨ init_rng å¹¶è®¾ç½®ç§å­ 1337ã€‚
init_rng = RNG(1337)

# åˆ›å»ºæ¨¡å‹å®ä¾‹ MLPRaw æˆ– MLPã€‚è¿™é‡Œé€‰æ‹©äº† MLPRawï¼Œå³æ‰‹åŠ¨å®ç°çš„æ¨¡å‹ç‰ˆæœ¬ã€‚
model = MLPRaw(vocab_size, context_length, embedding_size, hidden_size, init_rng)
# model = MLP(vocab_size, context_length, embedding_size, hidden_size, init_rng)

learning_rate = 7e-4 # è®¾ç½®å­¦ä¹ ç‡ learning_rateã€‚
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4) # åˆ›å»ºä¼˜åŒ–å™¨ AdamWï¼Œå¹¶æŒ‡å®šæ¨¡å‹å‚æ•°ã€å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡ç‡ã€‚
```

## è®­ç»ƒé…ç½®å’Œæ•°æ®åŠ è½½å™¨åˆå§‹åŒ–

é…ç½®è®­ç»ƒå‚æ•°ï¼ˆå¦‚æ‰¹æ¬¡å¤§å°å’Œè®­ç»ƒæ­¥æ•°ï¼‰ï¼Œåˆ›å»ºæ•°æ®åŠ è½½å™¨ã€‚

```python
timer = StepTimer() # åˆ›å»ºè®¡æ—¶å™¨ã€‚
batch_size = 128 # æ‰¹æ¬¡å¤§å°ã€‚
num_steps = 50000 # è®­ç»ƒæ­¥æ•°ã€‚
print(f'num_steps {num_steps}, num_epochs {num_steps * batch_size / len(train_tokens):.2f}') # æ‰“å°è®­ç»ƒæ­¥æ•°å’Œç›¸åº”çš„è®­ç»ƒå‘¨æœŸæ•°ã€‚
train_data_iter = dataloader(train_tokens, context_length, batch_size) # åˆ›å»ºæ•°æ®åŠ è½½å™¨
```

## è®­ç»ƒå¾ªç¯

æ‰§è¡Œè®­ç»ƒæ­¥éª¤ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡è°ƒåº¦ã€å‰å‘ä¼ æ’­ã€è®¡ç®—æŸå¤±ã€åå‘ä¼ æ’­ã€æ›´æ–°æ¨¡å‹å‚æ•°å’Œå®šæœŸè¯„ä¼°

```python
for step in range(num_steps):
    # ä½¿ç”¨ä½™å¼¦é€€ç«ç®—æ³•æ¥è°ƒæ•´å­¦ä¹ ç‡ã€‚
    lr = learning_rate * 0.5 * (1 + math.cos(math.pi * step / num_steps))
    # éå†ä¼˜åŒ–å™¨ä¸­çš„æ‰€æœ‰å‚æ•°ç»„ï¼Œæ›´æ–°å­¦ä¹ ç‡ã€‚
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    # æ¯éš” 200 æ­¥æˆ–åœ¨æœ€åä¸€æ­¥è¯„ä¼°ä¸€æ¬¡éªŒè¯æŸå¤±ã€‚
    last_step = step == num_steps - 1
    if step % 200 == 0 or last_step:    
        # è°ƒç”¨ eval_split å‡½æ•°è¯„ä¼°è®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®çš„æŸå¤±ã€‚
        train_loss = eval_split(model, train_tokens, max_batches=20)
        val_loss = eval_split(model, val_tokens)
        print(f'step {step:6d} | train_loss {train_loss:.6f} | val_loss {val_loss:.6f} | lr {lr:e} | time/step {timer.get_dt()*1000:.4f}ms')
    # ä½¿ç”¨è®¡æ—¶å™¨ timer è®°å½•æ‰€éœ€æ—¶é—´ã€‚
    with timer:
        # è·å–ä¸‹ä¸€ä¸ªè®­ç»ƒæ•°æ®æ‰¹æ¬¡ inputs å’Œ targetsã€‚
        inputs, targets = next(train_data_iter)
        # å‰å‘ä¼ æ’­ï¼Œè®¡ç®—æŸå¤± 
        logits, loss = model(inputs, targets)
        # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦ 
        loss.backward()
        # æ›´æ–°æ¨¡å‹å‚æ•°
        optimizer.step()
        # æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()
```

ä¸Šé¢ğŸ‘†ä»£ç ä¸­ç”¨åˆ°çš„ eval_split å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š

```python
@torch.inference_mode() # ä½¿ç”¨æ¨ç†æ¨¡å¼ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦å’Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚
def eval_split(model, tokens, max_batches=None):
    total_loss = 0
    num_batches = len(tokens) // batch_size
    if max_batches is not None:
        num_batches = min(num_batches, max_batches)
    data_iter = dataloader(tokens, context_length, batch_size)
    for _ in range(num_batches):
        inputs, targets = next(data_iter)
        logits, loss = model(inputs, targets)
        total_loss += loss.item() # loss.item() å°†æŸå¤±ä»å¼ é‡è½¬æ¢ä¸º Python æ ‡é‡ã€‚
    mean_loss = total_loss / num_batches # è®¡ç®—å¹³å‡æŸå¤±
    return mean_loss
```

# æ¨¡å‹æ¨ç†

åœ¨æµ‹è¯•é›†ä¸Šä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹å’ŒæŸå¤±è®¡ç®—ã€‚

```python
# æŒ‡å®šä¸€ä¸ªå›ºå®šçš„æç¤ºç¬¦ï¼Œä»è¯¥æç¤ºç¬¦å¼€å§‹ç”Ÿæˆåç»­æ–‡æœ¬ã€‚
sample_rng = RNG(42)
prompt = "\nrichard" # å®šä¹‰æç¤ºç¬¦å­—ç¬¦ä¸²
context = [char_to_token[c] for c in prompt] # å°†æç¤ºç¬¦ä¸­çš„å­—ç¬¦è½¬æ¢ä¸ºå¯¹åº”çš„ tokenã€‚
assert len(context) >= context_length # ç¡®ä¿æç¤ºç¬¦çš„é•¿åº¦è‡³å°‘ä¸º context_lengthã€‚
context = context[-context_length:] # æˆªå–æœ€å context_length ä¸ª tokenï¼Œç¡®ä¿ä¸Šä¸‹æ–‡é•¿åº¦ç¬¦åˆæ¨¡å‹è¦æ±‚ã€‚
print(prompt, end='', flush=True)
# é‡‡æ · 200 ä¸ªåç»­ token
with torch.inference_mode():
    for _ in range(200):
        context_tensor = torch.tensor(context).unsqueeze(0) # (1, T)
        logits, _ = model(context_tensor) # (1, V)
        probs = softmax(logits[0]) # (V, )ï¼Œ ä½¿ç”¨ softmax å‡½æ•°ä»¥å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒï¼Œå½¢çŠ¶ä¸º (V, )ã€‚
        coinf = sample_rng.random() # ç”Ÿæˆä¸€ä¸ªä»‹äº [0, 1) çš„éšæœºæµ®ç‚¹æ•°
        next_token = sample_discrete(probs, coinf) # æ ¹æ®æ¦‚ç‡åˆ†å¸ƒå’Œéšæœºæ•°é‡‡æ ·ä¸‹ä¸€ä¸ª tokenã€‚
        context = context[1:] + [next_token] # æ›´æ–°ä¸Šä¸‹æ–‡ï¼Œå°†æ–°ç”Ÿæˆçš„ token æ·»åŠ åˆ°ä¸Šä¸‹æ–‡æœ«å°¾ï¼Œå¹¶ç§»é™¤æœ€æ—©çš„ tokenã€‚
        print(token_to_char[next_token], end='', flush=True) # æ‰“å°æ–°ç”Ÿæˆçš„å­—ç¬¦ï¼Œä½†ä¸æ¢è¡Œï¼Œå¹¶åˆ·æ–°è¾“å‡ºç¼“å†²åŒºã€‚
print() # æ¢è¡Œ

# and finally report the test loss
test_loss = eval_split(model, test_tokens)
print(f'test_loss {test_loss}')
```

ä¸Šé¢ğŸ‘†ä»£ç ä¸­ä½¿ç”¨åˆ°çš„ softmax å‡½æ•°å’Œ sample_discrete å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š

```python
def softmax(logits):
    # logits æ˜¯å½¢çŠ¶ä¸º (V,) çš„ 1D å¼ é‡ã€‚
    maxval = torch.max(logits) # subtract max for numerical stability
    exps = torch.exp(logits - maxval)
    probs = exps / torch.sum(exps)
    return probs

def sample_discrete(probs, coinf): # ä»ç»™å®šçš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªç¦»æ•£å€¼ã€‚ç”¨äºæ¨¡æ‹Ÿéšæœºé‡‡æ ·è¿‡ç¨‹ã€‚
    cdf = 0.0 # åˆå§‹åŒ–ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (CDF) çš„åˆå§‹å€¼ä¸º 0ã€‚
    for i, prob in enumerate(probs):
        cdf += prob # ç´¯åŠ å½“å‰çš„æ¦‚ç‡å€¼åˆ° CDFã€‚
        if coinf < cdf: # å¦‚æœéšæœºæ•° coinf å°äºå½“å‰çš„ CDF å€¼ï¼Œè¿”å›å½“å‰ç´¢å¼• iã€‚
            return i    ## è¿™æ„å‘³ç€éšæœºæ•° coinf è½åœ¨å½“å‰æ¦‚ç‡åŒºé—´å†…ï¼Œé€‰æ‹©è¯¥ç´¢å¼•ä½œä¸ºé‡‡æ ·ç»“æœã€‚
    return len(probs) - 1  # å¦‚æœéå†å®Œæ‰€æœ‰çš„æ¦‚ç‡å€¼åä»æœªè¿”å›ï¼ˆå¯èƒ½ç”±äºæ•°å€¼è¯¯å·®ï¼‰ï¼Œè¿”å›æœ€åä¸€ä¸ªç´¢å¼•ã€‚ç”¨äºå¤„ç†è¾¹ç•Œæƒ…å†µã€‚
```

# è¿è¡Œ

æˆ‘ä»¬åœ¨ä¹¦ç”ŸÂ·æµ¦è¯­ç®—åŠ›å¹³å°ä¸­æ‰“å¼€å®ä¾‹åï¼Œè¿›å…¥ç›®å½• mlp ä¸‹ï¼Œé€‰æ‹©å¥½å¯¹åº”çš„æ¨¡å‹åï¼Œè¿è¡Œå‘½ä»¤ `python mlp_pytorch.py`å³å¯ã€‚

## è¿è¡Œç¯å¢ƒ

CPUï¼švCPU * 2000

å†…å­˜ï¼š24GB

GPUï¼š10% A100

æ˜¾å­˜: 8192MiB

## å‚æ•°é…ç½®

ç›´æ¥ä½¿ç”¨åˆå§‹å‚æ•°é…ç½®ï¼Œæœªè¿›è¡Œè°ƒæ•´ã€‚å¤§å®¶å¯ä»¥å°è¯•è°ƒæ•´å‚æ•°æ¥è·å¾—æ›´å¥½çš„æ¨¡å‹æ•ˆæœ~

## è¿è¡Œç»“æœ

1.  **MLPRaw**


![](https://files.mdnice.com/user/58235/3c5a7b4b-4fcb-4517-a99e-288c4f69cd89.png)


2.  **MLP**


![](https://files.mdnice.com/user/58235/2f9e36c2-ef0b-4323-8813-7e30804cbdd7.png)


å¯¹æ¯”ä¸Šé¢çš„è¿è¡Œç»“æœï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä¸€äº›æœ‰è¶£çš„åœ°æ–¹ï¼š

-   ä½¿ç”¨ nn.module æ¯”ä¸ä½¿ç”¨ nn.module æ„å»ºçš„æ¨¡å‹åœ¨ç›¸åŒé…ç½®ä¸‹è®­ç»ƒæ—¶é—´æ›´çŸ­ã€‚nn.Module æ˜¯ PyTorch æä¾›çš„ç”¨äºæ„å»ºç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ nn.Module å¯ä»¥å……åˆ†åˆ©ç”¨ PyTorch çš„ä¼˜åŒ–å’ŒåŠ é€Ÿæœºåˆ¶ã€‚
-   ç”±äºä½¿ç”¨äº†è‡ªå®šä¹‰çš„ RNG éšæœºæ•°æ¨¡å—è¿›è¡Œäº†éšæœºæ•°çš„æ§åˆ¶å’Œå‚æ•°æƒé‡çš„åˆå§‹åŒ–ï¼Œæ‰€ä»¥ä¸¤ç§æ¨¡å‹åœ¨æ¯æ¬¡è¯„ä¼°æ—¶å¾—åˆ°çš„æŸå¤±æ€»æ˜¯ä¸€è‡´çš„ã€‚

# ç»“è¯­

ä»¥ä¸Šå°±æ˜¯æœ¬æ¬¡ 101N0301 MLP Python æ ¸å¿ƒä»£ç ï¼ˆ pytorch ç‰ˆï¼‰è§£è¯»çš„å…¨éƒ¨å†…å®¹å•¦~

æ–‡ç« æœ‰ç‚¹é•¿ï¼Œå¹²è´§ä¹Ÿæœ‰ç‚¹å¤šï¼Œæ„Ÿè°¢çˆ±å­¦ä¹ çš„å¤§å®¶è¯»åˆ°æœ€å~

---

**LLM101n-CN å…±å»ºå…±å­¦è®¡åˆ’**æ˜¯ç”±æœºæ™ºæµè”åˆä¹¦ç”ŸÂ·æµ¦è¯­ç¤¾åŒºå…´è¶£å°ç»„å‘èµ· LLM101n ä¸­æ–‡ç‰ˆå…±å»ºå…±å­¦è®¡åˆ’ï¼Œæ—¨åœ¨å°†é¡¶çº§çš„ AI å­¦ä¹ èµ„æºå¸¦åˆ°ä¸­æ–‡ç¤¾åŒºã€‚åœ¨å…¬ä¼—å·åå°å›å¤ â€œ101nâ€ åŠ å…¥ LLM101n-CN å…±å»ºå…±å­¦è®¡åˆ’ï¼Œä¹ŸæœŸå¾…æ›´å¤šçš„å‹å¥½ç¤¾åŒºåˆä½œä¼™ä¼´åŠ å…¥æ­¤è®¡åˆ’ï¼ä¹Ÿæ¬¢è¿å…³æ³¨ä¸­æ–‡ç‰ˆ repoï¼š

<https://github.com/SmartFlowAI/LLM101n-CN>

