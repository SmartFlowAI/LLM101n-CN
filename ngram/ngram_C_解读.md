LLM101n æ˜¯ OpenAI è”åˆåˆ›å§‹äººã€â€œè®¡ç®—æœºè§†è§‰æ•™æ¯â€æé£é£æ•™æˆçš„é«˜å¾’Andrej Karpathy æ¨å‡ºçš„â€œä¸–ç•Œä¸Šæ˜¾ç„¶æœ€å¥½çš„ AI è¯¾ç¨‹â€ã€‚æœ¬æ–‡å°†å¸¦é¢†å¤§å®¶å¯¹ LLM101n ä¸­ ngram æ¨¡å—çš„ C è¯­è¨€æ ¸å¿ƒä»£ç é€å—è¿›è¡Œè§£è¯»ã€‚

æœ€æ–°æ•™ç¨‹è¯·è§ï¼š

[https://github.com/SmartFlowAI/LLM101n-CN/tree/master/ngram](https://github.com/SmartFlowAI/LLM101n-CN/tree/master/ngram)

å®Œæ•´ä»£ç ï¼š

[https://github.com/SmartFlowAI/LLM101n-CN/tree/master/ngram/ngram.c](https://github.com/SmartFlowAI/LLM101n-CN/tree/master/ngram/ngram.c)

åŸå§‹ä»£ç ä»“åº“ï¼š

[https://github.com/EurekaLabsAI/ngram](https://github.com/EurekaLabsAI/ngram)


# N-gram æ¨¡å‹ç®€ä»‹

N-gramæ˜¯ä¸€ç§åŸºäºç»Ÿè®¡è¯­è¨€æ¨¡å‹çš„ç®—æ³•ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯å°†æ–‡æœ¬é‡Œé¢çš„å†…å®¹æŒ‰ç…§å­—èŠ‚è¿›è¡Œå¤§å°ä¸º N çš„æ»‘åŠ¨çª—å£æ“ä½œï¼Œå½¢æˆäº†é•¿åº¦æ˜¯ N çš„å­—èŠ‚ç‰‡æ®µåºåˆ—ã€‚

æ¯ä¸€ä¸ªå­—èŠ‚ç‰‡æ®µç§°ä¸º gramï¼Œå¯¹æ‰€æœ‰ gram çš„å‡ºç°é¢‘åº¦è¿›è¡Œç»Ÿè®¡ï¼Œå¹¶ä¸”æŒ‰ç…§äº‹å…ˆè®¾å®šå¥½çš„é˜ˆå€¼è¿›è¡Œè¿‡æ»¤ï¼Œå½¢æˆå…³é”® gram åˆ—è¡¨ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªæ–‡æœ¬çš„å‘é‡ç‰¹å¾ç©ºé—´ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸€ç§ gram å°±æ˜¯ä¸€ä¸ªç‰¹å¾å‘é‡ç»´åº¦ã€‚

# å‰ç½®çŸ¥è¯†

æˆ‘ä»¬è¿™é‡Œè¦è®­ç»ƒçš„æ˜¯ä¸€ä¸ªç±»ä¼¼èŠ±åå†Œçš„ä¸œè¥¿ï¼Œåœ¨è®­ç»ƒæ•°æ®åœ¨ `data/train.txt` å†…ã€‚æœ€ç»ˆçš„ç›®çš„æ˜¯é€šè¿‡ç»™å®šå¤§å°çš„ N ï¼ˆæœ¬æ–‡å°† N è®¾ä¸º 1 åˆ° 6 è¿›è¡Œä¸¾ä¾‹ï¼‰ï¼Œä½¿ç”¨ N-Gram çš„ç®—æ³•æ¥è¿›è¡Œé‡‡æ ·ï¼Œä»¥ç”Ÿæˆä¸€ä¸ªæ–°çš„åå­—ã€‚

æ•´ä½“æ¥çœ‹å‘¢ï¼Œè¿™ç‰ˆ c ä»£ç ä¼šæ¯” python ä»£ç å¿«å¾ˆå¤šã€‚python è¿è¡Œæ—¶é•¿ä¸º 20sï¼Œè€Œ c ä»£ç ä»…ä¸º 1sã€‚

## ä»£ç ç¼–è¯‘

å†™å®Œ c ä»£ç éœ€è¦ç¼–è¯‘å†æ‰§è¡Œï¼Œç®€å•ç¼–è¯‘ä¸‹

```c
gcc -O3 ngram.c -o ngram -lm
```

å‚æ•°è§£é‡Šï¼š

-   -O3: ç”¨O3è¿›è¡Œç¼–è¯‘æœŸä¼˜åŒ–
-   -lmï¼šä½¿ç”¨æ•°å­¦ç›¸å…³çš„åº“

## è¶…å‚æ•°è®¾ç½®

ç¼–è¯‘å¥½ä¹‹åï¼Œå°±è¦è¿è¡Œå•¦ã€‚åœ¨è¿™é‡Œngramæ¨¡å‹æ¥å—ä¸¤ä¸ªè¾“å…¥:

-   seq_len: å®ƒè¡¨ç¤ºæ¨¡å‹ä½¿ç”¨å‡ ä¸ªå­—èŠ‚ç»„æˆä¸€ä¸ªç‰‡æ®µæ¥ç»Ÿè®¡æ¦‚ç‡ã€‚
-   Smoothing: å¹³æ»‘åº¦ã€‚å½“ä½¿ç”¨ N-gram æ¨¡å‹æ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ°æŸäº›å­—èŠ‚ç‰‡æ®µåœ¨è®­ç»ƒè¯­æ–™ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„æƒ…å†µï¼Œä»è€Œå¯¼è‡´æ¦‚ç‡ä¼°è®¡ä¸º 0ã€‚è€Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå³ä½¿æŸä¸ªå­—èŠ‚ç‰‡æ®µ æ²¡æœ‰åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°ï¼Œä¹Ÿä¸èƒ½ç®€å•åœ°è®¤ä¸ºå®ƒåœ¨æœªæ¥çš„æ–‡æœ¬ä¸­å‡ºç°çš„æ¦‚ç‡ä¸º 0ã€‚å› æ­¤å¹³æ»‘åº¦çš„ä½œç”¨æ˜¯è°ƒæ•´æ¦‚ç‡ä¼°è®¡ï¼Œ**ä½¿å¾—æ²¡æœ‰å‡ºç°è¿‡çš„å­—èŠ‚ç‰‡æ®µä¹Ÿèƒ½è·å¾—ä¸€ä¸ªéé›¶çš„æ¦‚ç‡**ï¼ŒåŒæ—¶å°½é‡ä¿æŒå¯¹å·²æœ‰æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ã€‚

### ï¼ˆä¸€ï¼‰é€‰æ‹©åˆé€‚çš„ seq_len

è®©æˆ‘ä»¬å…ˆè¯•è¯•ä¸åŒ`seq_len`çš„ç»“æœï¼š


![image](https://github.com/user-attachments/assets/2715c8d3-b49a-4889-a57e-791eb720cf17)


![image](https://github.com/user-attachments/assets/61d67a5f-9920-448a-a888-b30c56bdffed)


![image](https://github.com/user-attachments/assets/9f248229-ce80-428b-b687-f703ccc7fddf)




èƒ½çœ‹åˆ°åœ¨næ˜¯4çš„æ—¶å€™ï¼Œtext çš„ PPLï¼ˆå›°æƒ‘åº¦ï¼Œåæ˜ çœŸå®åˆ†å¸ƒä¸é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼‰æœ€ä½ã€‚ç”Ÿæˆçš„åå­—ä¹Ÿä¼šæ›´ç¬¦åˆè®¤çŸ¥ã€‚

### ï¼ˆäºŒï¼‰é€‰æ‹©åˆé€‚çš„ smoothing

è¿™é‡Œä¸å†å±•ç¤ºè°ƒå‚ç»“æœæˆªå›¾ï¼Œsmoothing å˜åŒ–æ—¶ï¼ŒPPLä¹Ÿä¼šéšä¹‹å˜åŒ–ã€‚ä»£ç é¢„è®¾çš„n = 4ï¼Œs=0.1å¾—åˆ°æœ€ä½çš„PPLã€‚

# ä»£ç è§£è¯»

## å…ˆä»ä¸»å‡½æ•°å¼€å§‹

åœ¨å®Œæ•´ä»£ç ä¸­ï¼Œä¸»å‡½æ•°ä»ç¬¬ 288 è¡Œå¼€å§‹

```c
// the arity of the n-gram model (1 = unigram, 2 = bigram, 3 = trigram, ...)
int seq_len = 4;
float smoothing = 0.1f;
```

è¿™é‡Œå®šä¹‰äº†ä¸¤ä¸ªæœ€é‡è¦çš„å˜é‡ `seq_len` å’Œ `smoothing`ï¼Œæ§åˆ¶æ¨¡å‹åˆ†ç‰‡å¤§å°å’Œå¹³æ»‘ç¨‹åº¦çš„å‚æ•°ã€‚

```c
// simple argparse, example usage: ./ngram -n 4 -s 0.1
for (int i = 1; i < argc; i+=2) {
    if (i + 1 >= argc) { error_usage(); } // must have arg after flag
    if (argv[i][0] != '-') { error_usage(); } // must start with dash
    if (!(strlen(argv[i]) == 2)) { error_usage(); } // must be -x (one dash, one letter)
    if (argv[i][1] == 'n') { seq_len = atoi(argv[i+1]); }
    else if (argv[i][1] == 's') { smoothing = atof(argv[i+1]); }
    else { error_usage(); }
}
```

ä»å‘½ä»¤è¡Œä¸­è¯»å–nå’Œsï¼Œé»˜è®¤ä¸ºn=4ï¼Œs=0.1ã€‚

## å®šä¹‰ NgramModel å¹¶åˆå§‹åŒ–

ç°åœ¨æˆ‘ä»¬å®šä½åˆ°ç¬¬ 305 è¡Œ

åœ¨ä¸»å‡½æ•°é‡Œï¼Œä»£ç æ¥ç€å£°æ˜äº†ä¸€ä¸ª`NgramModel`ç±»å‹çš„å˜é‡â€œmodelâ€ï¼Œç„¶åé€šè¿‡`ngram_init`å‡½æ•°å¯¹å…¶è¿›è¡Œåˆå§‹åŒ–ï¼Œåˆå§‹åŒ–æ—¶ä¼ å…¥äº†`NUM_TOKENS`ã€`seq_len`å’Œ`smoothing`ç­‰å‚æ•°ã€‚

```c
NgramModel model;
ngram_init(&model, NUM_TOKENS, seq_len, smoothing);
```

è®©æˆ‘ä»¬çœ‹çœ‹ `NgramModel` å’Œ `ngram_init` å…·ä½“æ˜¯ä»€ä¹ˆï¼Ÿ

### ï¼ˆä¸€ï¼‰å®šä¹‰NgramModel

æˆ‘ä»¬å®šä½åˆ°ç¬¬ 101 è¡Œ

```c
// ----------------------------------------------------------------------------
// ngram model

typedef struct {
    // hyperparameters
    int seq_len;
    int vocab_size;
    float smoothing;
    // parameters
    size_t num_counts; // size_t because int would only handle up to 2^31-1 ~= 2 billion counts
    uint32_t* counts;
    // internal buffer for ravel_index
    int* ravel_buffer;
} NgramModel;
```

`NgramModel` é€šè¿‡ä¸€ä¸ªç»“æ„ä½“å®šä¹‰ï¼Œåˆ†åˆ«æ˜¯ä¸‰ä¸ªè¶…å‚æ•°ï¼Œ`seq_len`, `vocab_size`(è¯è¡¨å¤§å°ï¼Œè¿™é‡Œæ˜¯26ä¸ªå­—ç¬¦+æˆªæ­¢ç¬¦),`smoothing`ã€‚ä¸‰ä¸ªå‚æ•°ï¼Œ`num_counts`, `counts`å’Œ`ravel_buffer`ã€‚è‡³äºå®ƒä»¬éƒ½æ˜¯ä»€ä¹ˆï¼Œè¯·ç»§ç»­å¾€ä¸‹çœ‹ã€‚

### ï¼ˆäºŒï¼‰åˆå§‹åŒ–NgramModel

æ¥åˆ°ç¬¬ 115 è¡Œ

```c
ngram_init(&model, NUM_TOKENS, seq_len, smoothing);
void ngram_init(NgramModel *model, const int vocab_size, const int seq_len, const float smoothing) {
    // sanity check and store the hyperparameters
    assert(vocab_size > 0);
    assert(seq_len >= 1 && seq_len <= 6); // sanity check max ngram size we'll handle
    model->vocab_size = vocab_size;
    model->seq_len = seq_len;
    model->smoothing = smoothing;
    // allocate and init memory for counts (np.zeros in numpy)
    model->num_counts = powi(vocab_size, seq_len);
    model->counts = (uint32_t*)mallocCheck(model->num_counts * sizeof(uint32_t));
    for (size_t i = 0; i < model->num_counts; i++) {
        model->counts[i] = 0;
    }
    // allocate buffer we will use for ravel_index
    model->ravel_buffer = (int*)mallocCheck(seq_len * sizeof(int));
}
```

åˆå§‹åŒ–æ—¶ï¼Œ`ngram_init` éœ€è¦ä¸€ä¸ªç”±ç±»å‹ä¸º `NgramModel` çš„ç»“æ„ä½“å®šä¹‰çš„æŒ‡é’ˆï¼Œä¸€ä¸ª `vocab_size`ï¼Œä¸€ä¸ª `seq_len`ï¼Œä¸€ä¸ª `smoothing`ã€‚å‡½æ•°é¦–å…ˆå¯¹è¾“å…¥çš„è¶…å‚æ•°ï¼ˆè¯æ±‡è¡¨å¤§å°ã€åºåˆ—é•¿åº¦å’Œå¹³æ»‘å‚æ•°ï¼‰è¿›è¡Œåˆç†æ€§æ£€æŸ¥ï¼Œç„¶åå°†è¿™äº›å‚æ•°å­˜å‚¨åœ¨æ¨¡å‹ç»“æ„ä½“ä¸­ã€‚æ¥ç€ï¼Œä¸ºè®¡æ•°åˆ†é…å†…å­˜å¹¶åˆå§‹åŒ–ä¸º 0ï¼Œè¿˜ä¸ºä¸€ä¸ªç”¨äºç´¢å¼•å¤„ç†çš„ç¼“å†²åŒºåˆ†é…å†…å­˜ã€‚æ•´ä¸ªå‡½æ•°çš„ç›®çš„æ˜¯ä¸º `NgramModel` ç»“æ„ä½“çš„ç›¸å…³æ•°æ®æˆå‘˜è¿›è¡Œæ­£ç¡®çš„åˆå§‹åŒ–å’Œå†…å­˜åˆ†é…ï¼Œä»¥å‡†å¤‡åç»­çš„æ“ä½œå’Œè®¡ç®—ã€‚

å…·ä½“è€Œè¨€ï¼Œé¦–å…ˆåšäº†ä¸€ç³»åˆ— assertï¼Œç„¶åï¼š

-  å®šä¹‰äº†æ¨¡å‹çš„ `vocab_size`ï¼ˆåœ¨åˆå§‹åŒ–`ngram_init`æ—¶`vocab_size`çš„å€¼è¢«è®¾ä¸ºäº† `NUM_TOKENS`ï¼Œä¹Ÿå°±æ˜¯åœ¨å®Œæ•´ä»£ç çš„ç¬¬ 83 è¡Œè®¾ç½®çš„ 27ï¼‰
-  å®šä¹‰äº† `seq_len` å’Œ `smoothing`ã€‚
-  å®šä¹‰äº† `num_counts` ä¸º $vocab\_size^{seq\_len}$ã€‚è¿™é‡Œ powi åŒç†äº std::powï¼Œå°±ä¸å±•å¼€äº†ã€‚
-  ç»™ counts åˆ†é…ç©ºé—´å¹¶å…¨éƒ¨åˆå§‹åŒ–ä¸º0

> ä¸Šé¢ä¸¤æ­¥çš„æ„Ÿè§‰å…¶å®å°±åƒæ˜¯torch.zeros([vocab_size, vocab_size, ..., vocab_size], dtype=torch.int32)ã€‚
>
> næ˜¯å‡ ç»´åº¦å°±æ˜¯å‡ ã€‚

-   ravel_bufferï¼Œ è¿™æ˜¯ä¸€ä¸ªå†…éƒ¨ç¼“å­˜ï¼Œåç»­è¯¦ç»†ä»‹ç»ã€‚

## ç¢°åˆ° DataLoader ç›¸å…³çš„ä¸€ç³»åˆ—æ“ä½œ

æ¥åˆ°ç¬¬ 308 è¡Œ

```c
// train the model
DataLoader train_loader;
dataloader_init(&train_loader, "data/train.txt", seq_len);
while (dataloader_next(&train_loader)) {
}
dataloader_free(&train_loader);
```

çœ‹è¿™æ®µå®ç°ï¼Œæˆ‘ä»¬é¦–å…ˆå®ä¾‹åŒ–äº†ä¸€ä¸ªtrain_loaderï¼Œä¹‹ååˆå§‹åŒ–äº†train_loaderã€‚ä¹‹åå¼€å§‹è®­ç»ƒï¼Œè®©æˆ‘ä»¬å¯¹ç…§å®ä¾‹åŒ–å’ŒåŸå§‹ä»£ç è¿›è¡Œè®²è§£ã€‚

### ï¼ˆä¸€ï¼‰å®šä¹‰DataLoader & å®šä¹‰Tape

æˆ‘ä»¬è·³è½¬åˆ°ç¬¬ 244 è¡Œ

```c
typedef struct {
    FILE *file;
    int seq_len;
    Tape tape;
} DataLoader;
```

å¾ˆå¥½ç†è§£çš„æ˜¯fileå’Œseq_lenã€‚é‚£Tapeæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿç®€å•ç†è§£å°±æ˜¯æŸä¸ªç‰‡æ®µã€‚

### ï¼ˆäºŒï¼‰å®šä¹‰ä¸€ä¸ªç¼“å†²åŒºTape

è§ç¬¬ 196 è¡Œ 

```c
typedef struct {
    int n;
    int length;
    int* buffer;
} Tape;
```

`Tape` å­˜å‚¨ä¸€ä¸ªå›ºå®šçª—å£çš„tokenï¼ŒåŠŸèƒ½ç±»ä¼¼äºä¸€ä¸ªæœ‰é™é˜Ÿåˆ—ã€‚ä»¥æ–¹ä¾¿Ngramæ¨¡å‹çš„æ›´æ–°ã€‚

-   N å½“å‰ç¼“å†²åŒºä¸­tokenæ•°é‡
-   lengthï¼šç¼“å†²åŒºé•¿åº¦
-   bufferï¼šç¼“å†²åŒº

### ï¼ˆä¸‰ï¼‰å®šä¹‰ DataLoader & Tape åˆå§‹åŒ–

è§å®Œæ•´ä»£ç çš„ç¬¬ 250 è¡Œ

```c
dataloader_init(&train_loader, "data/train.txt", seq_len);
void dataloader_init(DataLoader *dataloader, const char *path, const int seq_len) {
    dataloader->file = fopenCheck(path, "r");
    dataloader->seq_len = seq_len;
    tape_init(&dataloader->tape, seq_len);
}
```

é¦–å…ˆæˆ‘ä»¬è®©dataloaderè¯»å–äº†ä¸€ä¸ªæ–‡ä»¶ï¼Œä¹‹åç¡®å®šnå¤§å°ã€‚æœ€åç”¨seq_lenåˆå§‹åŒ–dataloaderä¸­çš„tapeã€‚

ä¸‹è§ç¬¬ 202 è¡Œ

```c
void tape_init(Tape *tape, const int length) {
    // we will allow a buffer of length 0, useful for the Unigram model
    assert(length >= 0);
    tape->length = length;
    tape->n = 0; // counts the number of elements in the buffer up to max
    tape->buffer = NULL;
    if (length > 0) {
        tape->buffer = (int*)mallocCheck(length * sizeof(int));
    }
}
```

é€šè¿‡seq_lenå®šä¹‰äº†tapeçš„lengthã€‚åˆå§‹åŒ–äº†æŸä¸ªç‰‡æ®µçš„ç»Ÿè®¡å€¼ï¼Œå¹¶åˆ†é…äº†lengthé•¿åº¦çš„ç©ºé—´ç»™tapeã€‚

### ï¼ˆå››ï¼‰è¿­ä»£DataLoader->dataloader_next & tape_update

è§ç¬¬ 256 è¡Œ

```c
dataloader_next(&train_loader)
int dataloader_next(DataLoader *dataloader) {
    // returns 1 if a new window was read, 0 if the end of the file was reached
    int c;
    while (1) {
        c = fgetc(dataloader->file);
        if (c == EOF) {
            break;
        }
        int token = tokenizer_encode(c);
        int ready = tape_update(&dataloader->tape, token);
        if (ready) {
            return 1;
        }
    }
    return 0;
}
```

-   **è¯»å–è¿‡ç¨‹**ï¼šè¯»å•Šè¯»ï¼Œå¦‚æœè¯»åˆ°äº†æ–‡ä»¶æœ€åå°± breakã€‚æ¯ä¸ªè¯»è¿›æ¥çš„å­—ç¬¦(\n, a,b,c...z)é€šè¿‡ tokenizer_encode è½¬æ¢æˆæ•°å­—(0-26)ã€‚æˆ‘ä»¬æŠŠ0å®šä¹‰æˆç»“æŸç¬¦å·,å¯¹åº”çš„å°±æ˜¯\nã€‚ï¼ˆå¾ˆåˆç†æ˜¯ä¸æ˜¯ï¼Œæ­£å¥½æ¢è¡Œæ˜¯ç»“æŸã€‚å¯ä»¥æ¢ä¸‹ä¸€ä¸ªè¯äº†ã€‚
-   **æ›´æ–°è¿‡ç¨‹**ï¼šï¼ˆè§ç¬¬ 219 è¡Œï¼‰

```c
int tape_update(Tape *tape, const int token) {
    // returns 1 if the tape is ready/full, 0 otherwise
    if (tape->length == 0) {
        return 1; // unigram tape is always ready
    }
    // shift all elements to the left by one
    for (int i = 0; i < tape->length - 1; i++) {
        tape->buffer[i] = tape->buffer[i + 1];
    }
    // add the new token to the end (on the right)
    tape->buffer[tape->length - 1] = token;
    // keep track of when we've filled the tape
    if (tape->n < tape->length) {
        tape->n++;
    }
    return (tape->n == tape->length);
```

å¼€å§‹çš„æ—¶å€™tapeç©ºç©ºå¦‚ä¹Ÿï¼Œæˆ‘ä»¬ä¸€ä¸ªä¸ªå­—ç¬¦ä»tapeçš„å³ä¾§è¯»è¿›æ¥ï¼Œç„¶åæ…¢æ…¢å…¥é˜Ÿã€‚ç­‰è¿™ä¸ªé˜Ÿåˆ—æ»¡äº†çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šè¿”å›return (tape->n == tape->length);ä¹Ÿå°±æ˜¯trueã€‚è¿™æ—¶å€™ä¼šè§¦å‘:

```c
if (ready) {
    return 1;
}
```

è¯´æ˜æˆ‘ä»¬ç¬¬ä¸€æ¬¡å †æ»¡è¿™ä¸ªé˜Ÿåˆ—ï¼Œé‚£ä¹ˆå°±å¯ä»¥å¼€å§‹æˆ‘ä»¬çš„æ›´æ–°æµç¨‹äº†ï½

### ï¼ˆäº”ï¼‰é‡Šæ”¾ DataLoader -> dataloader_free

```c
void dataloader_free(DataLoader *dataloader) {
    fclose(dataloader->file);
    tape_free(&dataloader->tape);
}
```

æ²¡å•¥å¥½è¯´çš„ï¼Œå…³é—­æ–‡ä»¶ï¼Œé‡Šæ”¾ç¼“å†²åŒºä¸€æ°”å‘µæˆï½

### ï¼ˆå…­ï¼‰å¼€å§‹è®­ç»ƒ

```c
ngram_train(&model, train_loader.tape.buffer);
```

è®­ç»ƒä»£ç å¼‚å¸¸çš„ç®€å•ï¼Œåªæœ‰ä¸€è¡Œã€‚è®©æˆ‘ä»¬çœ‹çœ‹åœ¨è¿™é‡Œéƒ½åšäº†å•¥å§ï¼ˆè§ç¬¬ 146 è¡Œï¼‰ã€‚

```c
ngram_train(&model, train_loader.tape.buffer);
void ngram_train(NgramModel *model, const int* tape) {
    // tape here is of length `seq_len`, and we want to update the counts
    size_t offset = ravel_index(tape, model->seq_len, model->vocab_size);
    assert(offset >= 0 && offset < model->num_counts);
    model->counts[offset]++;
}
```

æˆ‘ä»¬çš„è¾“å…¥æ˜¯æˆ‘ä»¬modelï¼Œå’Œä¸€ä¸ªè¢«å¡«æ»¡çš„é˜Ÿåˆ—ã€‚

### ï¼ˆä¸ƒï¼‰ravel_indexå¼€å§‹å¯»å€

è§ç¬¬ 132 è¡Œ

```c
size_t offset = ravel_index(tape, model->seq_len, model->vocab_size);
size_t ravel_index(const int* index, const int n, const int dim) {
    // convert an n-dimensional index into a 1D index (ravel_multi_index in numpy)
    // each index[i] is in the range [0, dim)
    size_t index1d = 0;
    size_t multiplier = 1;
    for (int i = n - 1; i >= 0; i--) {
        int ix = index[i];
        assert(ix >= 0 && ix < dim);
        index1d += multiplier * ix;
        multiplier *= dim;
    }
    return index1d;
}
```

æ ¹æ®nå’Œè¯è¡¨å¤§å°è¿›è¡Œ1Dè®­å€ï¼Œåšè¿™æ­¥æ˜¯å› ä¸ºcè¯­è¨€ä¸­é«˜ç»´æ•°ç»„å®šä¹‰å¯»å€è¾ƒå¤æ‚ï¼Œç›´æ¥å‹ç¼©æˆå•ç»´åº¦è¿›è¡Œå¯»å€ã€‚

-   åœ¨æ¯ä¸ªç»´åº¦ä¸Šå¯»æ‰¾ç´¢å¼•ï¼Œè¿™ä¸ªæ“ä½œåœ¨æ¯ä¸ªç»´åº¦ä¸Šå¯»æ‰¾äº†å®ƒæ‰€å¯¹åº”çš„ç´¢å¼•

```c
int ix = index[i];
```

-   ä¾æ¬¡è¿­ä»£ç»´åº¦

è¿™é‡Œå¯ä»¥ç†è§£æˆmultiplieræ˜¯ä¸€ä¸ªå¯»å€çš„åŸºï¼Œæ¯ä¸€æ¬¡è¿­ä»£æ–°çš„ç»´åº¦éœ€è¦ç»™åŸº * ä¸€ä¸ªè¯è¡¨å¤§å°ã€‚

```c
index1d += multiplier * ix;
multiplier *= dim;
```

æ„Ÿè§‰è¯´çš„è¿˜æ˜¯å¾ˆæŠ½è±¡ï¼Œé‚£è®©å’±ä»¬æ¥ç”»ä¸ªå›¾å§ï¼š

ä¸‹é¢æ˜¯ä¸€ä¸ª8x8çš„è¡¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³æ‰¾åˆ°è¿™ä¸ª3ï¼Œ3çš„åæ ‡ä¸€ç»´çš„è¡¨ç¤ºï¼Œæ˜¯ä¸æ˜¯éœ€è¦åƒè¿™æ ·

$$3Â +Â 3Â *Â 8Â =Â 27$$


![image](https://github.com/user-attachments/assets/d298586f-4dd2-42df-9308-d454d69c1dff)


æ‰©å±•åˆ°é«˜ç»´åº¦ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œå®ƒè¿”å›äº†ä¸€ä¸ªtokenåœ¨ä¸€ç»´ä¸Šçš„åç§»å€¼ã€‚ç„¶åæˆ‘ä»¬æŠŠè¿™ä¸ªåç§»å€¼åœ¨æ¨¡å‹å®šä¹‰çš„ä¸€ä¸ªcountsé‡Œ++ ... ä¸€ç›´è®­ç»ƒåˆ°ç»“æŸã€‚

## æ¨¡å‹æ¨ç†

ç°åœ¨æˆ‘ä»¬å›åˆ°ä¸»å‡½æ•°é‡Œï¼Œå¹¶è¿›å…¥æ¨¡å‹æ¨ç†éƒ¨åˆ†ï¼ˆä»ç¬¬ 319 è¡Œå¼€å§‹ï¼‰

```c
// sample from the model for 200 time steps
Tape sample_tape;
tape_init(&sample_tape, seq_len - 1);
tape_set(&sample_tape, EOT_TOKEN); // fill with EOT tokens to init
uint64_t rng = 1337;
for (int i = 0; i < 200; i++) {
    ngram_inference(&model, sample_tape.buffer, probs);
    float coinf = random_f32(&rng);
    int token = sample_discrete(probs, NUM_TOKENS, coinf);
    tape_update(&sample_tape, token);
    char c = tokenizer_decode(token);
    printf("%c", c);
}
printf("\n");
```

æ˜¯ä¸æ˜¯çœ‹å®Œäº†è®­ç»ƒï¼Œæ¨ç†éƒ½çœ‹èµ·æ¥çœ‰æ¸…ç›®ç§€äº†ï½åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬å…ˆå®šä¹‰äº†ä¸€ä¸ªTapeï¼Œå¡«å……æ»¡ç»“æŸç¬¦å·ï¼Œä¹Ÿå°±æ˜¯0

```c
ngram_inference(&model, sample_tape.buffer, probs);
void ngram_inference(NgramModel *model, const int* tape, float* probs) {
    // here, tape is of length `seq_len - 1`, and we want to predict the next token
    // probs should be a pre-allocated buffer of size `vocab_size`

    // copy the tape into the buffer and set the last element to zero
    for (int i = 0; i < model->seq_len - 1; i++) {
        model->ravel_buffer[i] = tape[i];
    }
    ...
}
```

ravel_bufferåœ¨æ¨ç†æ—¶ä½¿ç”¨ï¼Œæˆ‘ä»¬ç»™ravel_bufferæœ€åå¡«ä¸Š0å…ƒç´ ã€‚

```c
model->ravel_buffer[model->seq_len - 1] = 0;
// find the offset into the counts array based on the context
size_t offset = ravel_index(model->ravel_buffer, model->seq_len, model->vocab_size);
// seek to the row of counts for this context
uint32_t* counts_row = model->counts + offset;
```

åœ¨è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªç‚¹ï¼Œæ¯æ¬¡ravel_bufferçš„æœ€åä¸€ä¸ªå€¼éƒ½æ˜¯0ï¼Œè¿™æ ·ç´¢å¼•æ—¶æ‰¾åˆ°çš„offsetéƒ½æ˜¯æŸä¸€è¡Œçš„å¼€å§‹ã€‚counts_rowèƒ½æ‰¾åˆ°ç‰¹å®šè¡Œçš„å¼€å§‹ä½ç½®ã€‚

```c
// calculate the sum of counts in the row
float row_sum = model->vocab_size * model->smoothing;
for (int i = 0; i < model->vocab_size; i++) {
    row_sum += counts_row[i];
}
```

$$row\_sumÂ +Â smootingÂ *Â vocab$$

æ·»åŠ smootingä¸ºäº†é˜²æ­¢row_sumä¸º0çš„æƒ…å†µå‘ç”Ÿã€‚

```c
if (row_sum == 0.0f) {
    // the entire row of counts is zero, so let's set uniform probabilities
    float uniform_prob = 1.0f / model->vocab_size;
    for (int i = 0; i < model->vocab_size; i++) {
        probs[i] = uniform_prob;
    }
} else {
    // normalize the row of counts into probabilities
    float scale = 1.0f / row_sum;
    for (int i = 0; i < model->vocab_size; i++) {
        float counts_i = counts_row[i] + model->smoothing;
        probs[i] = scale * counts_i;
    }
}
```

å½“ç„¶å¦‚æœä½ çš„smoothingè®¾æˆ0äº†ï¼Œé‚£ä¸å¥½æ„æ€ã€‚è¿˜æ˜¯ä¼šä¸º0ï¼Œè¿™æ—¶å€™å°±æ¯ä¸ªä½ç½®å‡åˆ†ã€‚å¦åˆ™è®¡ç®—è¿™ä¸€ç»„tapeæ¯ä¸ªå­—ç¬¦çš„æ¦‚ç‡ã€‚æ‹¿n=4ä¸¾ä¾‹ï¼Œç°åœ¨è¿™æ­¥æˆ‘ä»¬ç®—å¾—å°±æ˜¯ï¼š

$$p(new)Â =Â p(new|0, 0, 0, 0)Â ifÂ nÂ ==Â 4$$

ä¹‹åæˆ‘ä»¬ä¼šåœ¨è¿™ç»„æ¦‚ç‡é‡Œéšæœºé‡‡æ ·ä¸€ä¸‹ã€‚é‡‡æ ·å°±ä¸å¤šè¯´äº†ğŸ˜‹

```c
int token = sample_discrete(probs, NUM_TOKENS, coinf);
```

é‡‡æ ·å®Œä¹‹åè¿˜æœ‰ä»€ä¹ˆå¾ˆå…³é”®ï¼Ÿé‚£å°±æ˜¯æŠŠé‡‡æ ·çš„ç»“æœåŠ è¿›tapeé‡Œå•Šï¼

```c
tape_update(&sample_tape, token);
```

è¿™æ ·ä¸‹æ¬¡é¢„æµ‹å°±ä¼šæ ¹æ®è¿™æ¬¡çš„ç»“æœå†è¿›è¡Œé¢„æµ‹å•¦ï¼Œä¸€ç›´é¢„æµ‹åˆ°ç”Ÿæˆä¸€ä¸ª0ã€‚0æ˜¯æˆªæ­¢ç¬¦å·ï¼Œä¹Ÿæ˜¯æ¢è¡Œç¬¦å·ã€‚æˆ‘ä»¬å°±ä¼šå¼€å§‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚

## æ¨¡å‹æµ‹è¯•

æœ€åè¿˜æ˜¯è¦è®²è®²æµ‹è¯•ï¼ˆä»ç¬¬ 334 è¡Œå¼€å§‹ï¼‰ï¼Œå¤§éƒ¨åˆ†çš„æ¥å£éƒ½åœ¨ä¸Šæ–‡åŒæ­¥è¿‡äº†ã€‚

```c
// evaluate the test split loss
DataLoader test_loader;
dataloader_init(&test_loader, "data/test.txt", seq_len);
float sum_loss = 0.0f;
int count = 0;
while (dataloader_next(&test_loader)) {
    // note that ngram_inference will only use the first seq_len - 1 tokens in buffer
    ngram_inference(&model, test_loader.tape.buffer, probs);
    // and the last token in the tape buffer is the label
    int target = test_loader.tape.buffer[seq_len - 1];
    // negative log likelihood loss
    sum_loss += -logf(probs[target]);
    count++;
}
dataloader_free(&test_loader);
float mean_loss = sum_loss / count;
float test_perplexity = expf(mean_loss);
printf("test_loss %f, test_perplexity %f\n", mean_loss, test_perplexity);
```

è¿™é‡Œæˆ‘ä»¬ä¸»è¦æ˜¯è®¡ç®—ä¸€ä¸ªnegative log likelihood lossã€‚å®ƒçš„è®¡ç®—æµç¨‹æ˜¯è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ¯æ¬¡teståŠ è½½ä¸€ä¸ªæ–°çš„å­—ç¬¦ã€‚å¹¶è¿›è¡Œç”Ÿæˆã€‚æ‰¾åˆ°æ–°çš„å­—ç¬¦åº”è¯¥æ˜¯ä»€ä¹ˆã€‚

```c
int target = test_loader.tape.buffer[seq_len - 1];
```

å¹¶å¾—åˆ°è¿™ä¸ªå­—ç¬¦çš„negative log likelihood loss

```c
-logf(probs[target]);
```

ä¹‹åä¸€ç›´ç´¯åŠ å°±å¥½äº†ï¼Œè®¡ç®—å¹³å‡losså’ŒPPL

```c
float mean_loss = sum_loss / count;
float test_perplexity = expf(mean_loss);
```

# ç»“è¯­

è¿™å°±æ˜¯å…³äºngram cçš„å…¨éƒ¨å•¦ï¼Œåˆæ˜¯è‚åˆ°å¤©äº®çš„ä¸€æ¬¡ã€‚å­¦ä¹ cçš„ä»£ç èƒ½è®©ä½ æ›´æ„Ÿè§‰åˆ°èä¼šè´¯é€šï¼Œæ›´åŠ æŒæ¡ngramã€‚ç¡®å®å¾ˆæœ‰æ„æ€ï½

# LLM101n-CN å…±å»ºå…±å­¦è®¡åˆ’

**LLM101n-CN å…±å»ºå…±å­¦è®¡åˆ’**æ˜¯ç”±æœºæ™ºæµè”åˆä¹¦ç”ŸÂ·æµ¦è¯­ç¤¾åŒºå…´è¶£å°ç»„å‘èµ· LLM101n ä¸­æ–‡ç‰ˆå…±å»ºå…±å­¦è®¡åˆ’ï¼Œæ—¨åœ¨å°†é¡¶çº§çš„ AI å­¦ä¹ èµ„æºå¸¦åˆ°ä¸­æ–‡ç¤¾åŒºã€‚åœ¨â€œæœºæ™ºæµâ€å…¬ä¼—å·åå°å›å¤ â€œ**101n**â€ åŠ å…¥ LLM101n-CN å…±å»ºå…±å­¦è®¡åˆ’ï¼Œä¹ŸæœŸå¾…æ›´å¤šçš„å‹å¥½ç¤¾åŒºåˆä½œä¼™ä¼´åŠ å…¥æ­¤è®¡åˆ’ï¼ä¹Ÿæ¬¢è¿å…³æ³¨æˆ‘ä»¬çš„ä¸­æ–‡ç‰ˆ repoï¼š

[https://github.com/SmartFlowAI/LLM101n-CN](https://github.com/SmartFlowAI/LLM101n-CN)
